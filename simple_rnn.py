# -*- coding: utf-8 -*-
"""simple RNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oOLfhFe2zBqyLM6cRhZ9XCHFyOntsx1B
"""

import tensorflow as tf
print(tf.__version__)

import numpy as np
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding,SimpleRNN,Dense,Dropout,LSTM
from tensorflow.keras.callbacks import EarlyStopping

#load the imdb dataset
max_features=10000 #vocabulary size
max_len=500
(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=max_features)


# print shape of the data
print(f'Training data shape:{x_train.shape},Training labels shape:{y_train.shape}')
print(f'Testing data shape:{x_test.shape},Testing labels shape:{y_test.shape}')

# inspect a sample review and its labels
sample_review=x_train[0]
sample_label=y_train[0]

print(f"Sample review(as integers):{sample_review}")
print(f"Sample label:{sample_label}")

#mappping of words index bacl to words(for understanding)
word_index=imdb.get_word_index()
#word_index
reverse_word_index={value:key for key,value in word_index.items()}
reverse_word_index

decoded_review=' '.join([reverse_word_index.get(i - 3,'?')for i in sample_review])
decoded_review

x_train=sequence.pad_sequences(x_train,maxlen=max_len)
x_test=sequence.pad_sequences(x_test,maxlen=max_len)

model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=500),  # Define input length here
    LSTM(128, activation='tanh', return_sequences=False),  # Use LSTM instead of SimpleRNN
    Dropout(0.5),  # Dropout for regularization
    Dense(1, activation='sigmoid')  # Output layer
])

# Manually build the model to force parameter allocation
model.build(input_shape=(None, max_len))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

#create an instance of EarlyStopping Callback
early_stopping=EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)
early_stopping

#train the modelwith early stopping
history=model.fit(
    x_train,y_train,epochs=10,batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping]
)

# Evaluate the model
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

model.save('simple_rnn_imdb.h5')

#step-1:import Libraries and Load the Model
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import load_model

#Load the IMDB dataset word index
word_index =imdb.get_word_index()
reverse_word_index = {value:key for key,value in word_index.items()}

#Load the pre-trained model with Tanh activation
model=load_model('simple_rnn_imdb.h5')

#step-2:Helper Functions
#Functionto decode reviews
def decode_review(encoded_review):
  return' '.join([reverse_word_index.get(i - 3,'?')for i in encoded_review])

#Function to preprocess user input
def preprocess_text(text):
  words=text.lower().split()
  encoded_review = [word_index.get(word,2)+3 for word in words]
  padded_review = sequence.pad_sequences([encoded_review],maxlen=500)
  return padded_review

#step-3:Prediction function

def predict_sentiment(review):
  preprocessing_input=preprocess_text(review)

  prediction=model.predict(preprocessing_input)

  sentiment = 'Positive' if prediction[0][0] > 0.5 else 'Negative'

  return sentiment,prediction[0][0]

#step-4: user input and prediction
# example review for prediction
example_review = "The movie was fantastic! The acting was great and the plot was thrilling."

sentiment,score=predict_sentiment(example_review)

print(f'Review:{example_review}')
print(f'Sentiment:{sentiment}')
print(f'Prediction Score:{score}')